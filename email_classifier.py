# -*- coding: utf-8 -*-
"""Email_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZBt3jyBTtBQsxqyDof_mED41KP-hjeOB
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df=pd.read_csv("/content/drive/MyDrive/MACHINE LEARNING /Data_Sets/spam (1).csv")

df

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
df['Category']=encoder.fit_transform(df['Category'])

df.isnull().sum()

df.duplicated().sum()

df=df.drop_duplicates(keep='first')

df.head()

"""EDA"""

import matplotlib.pyplot as plt
plt.pie(df['Category'].value_counts(),labels=['ham','spam'],autopct='%0.2f')
plt.show()

!pip install nltk

import nltk

nltk.download('punkt')

df['num_characters']=df['Message'].apply(len)

df

df['num_words']=df['Message'].apply(lambda x: len(nltk.word_tokenize(x)))

df['num_sentence']=df['Message'].apply(lambda x:len(nltk.sent_tokenize(x)))

df

df[['num_characters','num_words','num_sentence']].describe()

df[df['Category']==0][['num_characters','num_words','num_sentence']].describe()

df[df['Category']==1][['num_characters','num_words','num_sentence']].describe()

import seaborn as sns

sns.histplot(df[df['Category']==0]['num_characters'])
sns.histplot(df[df['Category']==1]['num_characters'],color='red')

sns.histplot(df[df['Category']==0]['num_words'])
sns.histplot(df[df['Category']==1]['num_words'],color='red')

sns.histplot(df[df['Category']==0]['num_sentence'])
sns.histplot(df[df['Category']==1]['num_sentence'],color='red')

sns.pairplot(df,hue='Category')

from nltk.corpus import stopwords

import string

from nltk.stem import PorterStemmer

ps = PorterStemmer()

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from nltk.stem import PorterStemmer


nltk.download('stopwords')
nltk.download('punkt')


ps = PorterStemmer()

def transform_text(text):
    text = text.lower()
    text = word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        y.append(ps.stem(i))

    return " ".join(y)






df['transformed_text'] = df['Message'].apply(transform_text)

df

from wordcloud import WordCloud
wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')

spam_wc=wc.generate(df[df['Category']==1]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(12,6))
plt.imshow(spam_wc)

ham_wc=wc.generate(df[df['Category']==0]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(12,6))
plt.imshow(spam_wc)

spam_corpus=[]
for msg in df[df['Category']==1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

word_counts = Counter(spam_corpus).most_common(30)

word_df = pd.DataFrame(word_counts, columns=['word', 'count'])

plt.figure(figsize=(10, 6))
sns.barplot(x='word', y='count', data=word_df)

plt.xticks(rotation='vertical')
plt.xlabel('Words')
plt.ylabel('Counts')
plt.title('Top 30 Most Common Words in Spam Corpus')
plt.show()

ham_corpus=[]
for msg in df[df['Category']==0]['transformed_text'].tolist():
  for word in msg.split():
    ham_corpus.append(word)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

word_counts_1 = Counter(ham_corpus).most_common(30)

word_df_1 = pd.DataFrame(word_counts, columns=['word', 'count'])

plt.figure(figsize=(10, 6))
sns.barplot(x='word', y='count', data=word_df_1)

plt.xticks(rotation='vertical')
plt.xlabel('Words')
plt.ylabel('Counts')
plt.title('Top 30 Most Common Words in ham Corpus')
plt.show()

"""MODEL BUILDING

"""

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer()

X=cv.fit_transform(df['transformed_text']).toarray()

X

y=df['Category'].values

y

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb.fit(X_train,y_train)
y_pred1=gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(X_train,y_train)
y_pred2=mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

bnb.fit(X_train,y_train)
y_pred3=mnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

import pickle

# Train the Bernoulli Naive Bayes model (assuming bnb is already trained)
bnb.fit(X_train, y_train)

# Save the trained model to a file using pickle
with open('bernoulli_model.pkl', 'wb') as f:
    pickle.dump(bnb, f)

with open('count_vectorizer.pkl', 'wb') as f:
    pickle.dump(cv, f)